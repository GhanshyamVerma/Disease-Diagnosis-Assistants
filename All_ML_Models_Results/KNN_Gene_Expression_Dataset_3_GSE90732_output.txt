[1] "Starting KNN learning using training data"[1] "KNN training results:"k-Nearest Neighbors    14 samples10734 predictors    2 classes: 'Not RVI', 'RVI' No pre-processingResampling: Bootstrapped (25 reps) Summary of sample sizes: 14, 14, 14, 14, 14, 14, ... Resampling results across tuning parameters:  k   Accuracy   Kappa           1  0.3415238  -1.513896e-01   2  0.4276190   1.476231e-02   3  0.4516190   4.573659e-02   4  0.4293333  -2.042890e-02   5  0.4696190   9.287619e-02   6  0.5193333   1.445841e-01   7  0.4566667   6.583342e-02   8  0.4405714   7.363503e-02   9  0.4616190   1.391286e-01  10  0.4156190   6.707532e-02  11  0.4012381   7.058941e-02  12  0.3845714   2.285714e-02  13  0.3619048   0.000000e+00  14  0.3619048  -2.000000e-02  15  0.3619048  -2.000000e-02  16  0.3619048  -2.000000e-02  17  0.3619048  -2.000000e-02  18  0.3519048  -4.000000e-02  19  0.3719048  -4.440892e-18  20  0.3419048  -4.000000e-02  21  0.3619048   0.000000e+00  22  0.3619048  -2.000000e-02  23  0.3585714  -1.200000e-02  24  0.3519048  -2.000000e-02  25  0.3385714  -5.200000e-02  26  0.3519048  -4.000000e-02  27  0.3652381  -2.000000e-02  28  0.3285714  -7.200000e-02  29  0.3385714  -5.200000e-02  30  0.3785714   2.000000e-02Accuracy was used to select the optimal model using the largest value.The final value used for the model was k = 6.[1] "Starting validation process:"[1] "KNN validation results:"k-Nearest Neighbors     8 samples10734 predictors    2 classes: 'Not RVI', 'RVI' No pre-processingResampling: Bootstrapped (25 reps) Summary of sample sizes: 8, 8, 8, 8, 8, 8, ... Resampling results across tuning parameters:  k   Accuracy   Kappa           1  0.6033333   2.391304e-01   2  0.5733333   1.739130e-01   3  0.5066667   7.391304e-02   4  0.5733333   2.130435e-01   5  0.5700000   1.739130e-01   6  0.4766667   1.125000e-01   7  0.3500000   2.000000e-02   8  0.3000000  -7.200000e-02   9  0.3433333  -4.440892e-18  10  0.3300000  -2.000000e-02  11  0.3733333   6.000000e-02  12  0.3100000  -6.000000e-02  13  0.3666667   5.600000e-02  14  0.3400000   0.000000e+00  15  0.2933333  -1.000000e-01  16  0.3266667  -2.400000e-02  17  0.3466667   0.000000e+00  18  0.3266667  -4.000000e-02  19  0.2733333  -1.400000e-01  20  0.3566667   3.600000e-02  21  0.2900000  -9.200000e-02  22  0.3666667   5.600000e-02  23  0.3566667   3.600000e-02  24  0.3600000   4.800000e-02  25  0.3266667  -4.000000e-02  26  0.3133333  -6.000000e-02  27  0.2933333  -1.000000e-01  28  0.3500000   2.000000e-02  29  0.3566667   3.600000e-02  30  0.3666667   4.000000e-02Accuracy was used to select the optimal model using the largest value.The final value used for the model was k = 1.[1] "Final value of k: 1"[1] "Starting prediction for holdout testset:"[1] "Writing results to /Users/ghanshyam/My_Documents/Year_2023/BMC_Bioinformatics_2023/GitHub/Disease-Diagnosis-Assistants/All_ML_Models_Results/"[1] "Print confusion matrix for hold-out test set for KNN for Gene_Expression_Dataset_3_GSE90732:"Confusion Matrix and Statistics          ReferencePrediction Not RVI RVI   Not RVI       0   1   RVI           0   3                                                         Accuracy : 0.75                             95% CI : (0.1941, 0.9937)    No Information Rate : 1                   P-Value [Acc > NIR] : 1                                                                           Kappa : 0                                                          Mcnemar's Test P-Value : 1                                                                     Sensitivity :   NA                        Specificity : 0.75                     Pos Pred Value :   NA                     Neg Pred Value :   NA                         Prevalence : 0.00                     Detection Rate : 0.00               Detection Prevalence : 0.25                  Balanced Accuracy :   NA                                                             'Positive' Class : Not RVI